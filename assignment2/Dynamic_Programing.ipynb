{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "from collections import deque\n",
    "import pyprind\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Transition Probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "P_a0 = np.array([\n",
    "        [0.5, 0.5],\n",
    "        [0.0, 1.0]\n",
    "    ])\n",
    "P_a1 = np.array([\n",
    "        [0.0, 1.0],\n",
    "        [0.0, 1.0]\n",
    "    ])\n",
    "P_a2 = np.array([\n",
    "        [1.0, 0.0],\n",
    "        [0.0, 1.0]\n",
    "    ])\n",
    "P_all = np.concatenate([P_a0, P_a1, P_a2], axis=0)\n",
    "P_list = [P_a0, P_a1, P_a2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Reward Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "R_all = np.array([\n",
    "        [5.0, 10.0, -1000],\n",
    "        [-1000, -1000, -1.0]\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting discount factor $\\gamma$ and tolerance Bellman Error $\\tau$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gamma = 0.95\n",
    "tau = 1e-10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing Value Function Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "V = np.zeros(2)\n",
    "policy = np.zeros(2, dtype=int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value Iteration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged after 457 iterations with error 9.828401969972982e-11\n",
      "Time taken: 0.021943092346191406 s\n",
      "Optimal Values: [ -8.57142857 -20.        ]\n",
      "Optimal Policy: [0 2]\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "for i in range(1000):\n",
    "    V_old = np.copy(V)\n",
    "    V = np.max(R_all + gamma*np.dot(P_all, V).reshape(R_all.shape, order='F'), axis=1)\n",
    "    policy = np.argmax(R_all + gamma*np.dot(P_all, V).reshape(R_all.shape, order='F'), axis=1)\n",
    "    error = np.linalg.norm(V-V_old, 2)\n",
    "    if error < tau:\n",
    "        print('Converged after {:} iterations with error {:}'.format(i+1, error))\n",
    "        break\n",
    "print('Time taken:', time() - start, 's')\n",
    "print('Optimal Values:',V)\n",
    "print('Optimal Policy:',policy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanity check by calculating Action Value function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 0.0550079345703125 s\n",
      "Optimal Values [ -8.57142857 -20.        ]\n"
     ]
    }
   ],
   "source": [
    "Q = -1000*np.ones((2,3))\n",
    "A = [[0, 1], [2]]\n",
    "start = time()\n",
    "for i in range(1000):\n",
    "    for s in range(2):\n",
    "        for a in A[s]:\n",
    "            res = R_all[s, a]\n",
    "            for sprime in range(2):\n",
    "                res += gamma*P_list[a][s, sprime]*np.max(Q[sprime])\n",
    "            Q[s,a] = res\n",
    "V = np.max(Q, axis=1)\n",
    "print('Time taken:', time() - start, 's')\n",
    "print('Optimal Values',V)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged after 2 iterations\n",
      "Time taken: 0.0012879371643066406 s\n",
      "Optimal Policy: [0 2]\n",
      "Optimal Values: [ -8.57142857 -20.        ]\n"
     ]
    }
   ],
   "source": [
    "V_pi = np.zeros(2)\n",
    "pi = np.zeros(2, dtype=int)\n",
    "start = time()\n",
    "for i in range(900000):\n",
    "    pi_old = np.copy(pi)\n",
    "    P_pi = np.array([P_list[pi[0]][0],P_list[pi[1]][1]])\n",
    "    R_pi = np.array([R_all[0, pi[0]], R_all[1, pi[1]]])\n",
    "    V_pi = np.linalg.solve(a=np.eye(len(P_pi))-gamma*P_pi, b=R_pi) #Policy Iteration\n",
    "    pi = np.argmax(R_all + gamma*np.dot(P_all, V_pi).reshape(R_all.shape, order='F'), axis=1)\n",
    "    if np.allclose(pi, pi_old):\n",
    "        print('Converged after {:} iterations'.format(i+1))\n",
    "        break\n",
    "        pass\n",
    "print('Time taken:', time() - start, 's')\n",
    "print('Optimal Policy:',pi)\n",
    "print('Optimal Values:',V_pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modified Policy Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged after 2 iterations\n",
      "Time taken: 81.76924300193787 s\n",
      "Optimal Policy: [0 2]\n",
      "Optimal Values: [-467.625 -951.   ]\n"
     ]
    }
   ],
   "source": [
    "V_pi2 = np.zeros(2)\n",
    "pi2 = np.zeros(2, dtype=int)\n",
    "for i in range(900000):\n",
    "    pi_old = np.copy(pi2)\n",
    "    P_pi = np.array([P_list[pi2[0]][0],P_list[pi2[1]][1]])\n",
    "    R_pi = np.array([R_all[0, pi2[0]], R_all[1, pi2[1]]])\n",
    "   \n",
    "    V_pi2 = R_pi + gamma*np.dot(P_pi, V_pi2) #Modified Policy Iteration\n",
    "    pi2 = np.argmax(R_all + gamma*np.dot(P_all, V_pi2).reshape(R_all.shape, order='F'), axis=1)\n",
    "    if np.allclose(pi2, pi_old):\n",
    "        print('Converged after {:} iterations'.format(i+1))\n",
    "        break\n",
    "        pass\n",
    "print('Time taken:', time() - start, 's')\n",
    "print('Optimal Policy:',pi2)\n",
    "print('Optimal Values:',V_pi2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Comment on Results\n",
    "   It is very difficult to draw concrete conclusions from the results of this MDP since the action and state space is very small. Discussion on the results will hence be done in the next notebook which uses a environment with a larger action and state space \n",
    "   \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/anilpatil/anaconda/bin/python\r\n"
     ]
    }
   ],
   "source": [
    "!which python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"FrozenLake-v0\")\n",
    "#env = gym.make(\"Taxi-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N = 10000\n",
    "Q = np.zeros((env.env.nS, env.env.nA))\n",
    "gamma = 0.1\n",
    "tol = 1e-6\n",
    "bar = pyprind.ProgBar(N)\n",
    "for i in range(N):\n",
    "    bar.update()\n",
    "    Q_old = np.copy(Q)\n",
    "    for s in range(env.env.nS):\n",
    "        for a in list(env.env.P[s].keys()):\n",
    "            res = 0.0\n",
    "            for j, sprime in enumerate([k[1] for k in env.env.P[s][a]]):\n",
    "                res += env.env.P[s][a][j][0]*(env.env.P[s][a][j][2] + gamma*np.max(Q[sprime]))\n",
    "            Q[s,a] = res\n",
    "    error = np.linalg.norm(Q-Q_old,2)\n",
    "    if error < tol:\n",
    "        print('Converged after {:} iterations and error is {:}'.format(i+1,error))\n",
    "        break\n",
    "print(bar)\n",
    "V = np.max(Q, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env.env.P[5].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env.env.nS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env.env.nS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "V_policy = np.zeros(env.env.nS)\n",
    "policy = np.zeros(env.env.nS, dtype=int)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "env.env.P[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "nA, nS = env.env.nA, env.env.nS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "T = np.zeros([nS, nA, nS])\n",
    "R = np.zeros([nS, nA, nS])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for s in range(nS):\n",
    "    for a in range(nA):\n",
    "        transitions = env.env.P[s][a]\n",
    "        for p_trans,next_s,rew,done in transitions:\n",
    "#             print(next_s)\n",
    "            T[s,a,next_s] += p_trans\n",
    "            \n",
    "            R[s,a,next_s] = rew\n",
    "        T[s,a,:]/=np.sum(T[s,a,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for s in range(nS):\n",
    "#      for a in range(nA):\n",
    "print(env.env.P[10][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "P_list = [np.zeros([nS,nS]) for _ in range(nA)]\n",
    "R_list = [-1000*np.ones((nS, nS)) for _ in range(nA)]\n",
    "for s in range(nS):\n",
    "    for a in range(nA):\n",
    "        transitions = env.env.P[s][a]\n",
    "        for p_trans,next_s,rew,done in transitions:\n",
    "            P_list[a][s,next_s]= p_trans\n",
    "            R_list[a][s,next_s] = rew\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "R_all = np.vstack([np.sum(R_list[i]*P_list[i], axis=1) for i in range(nA)]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "R_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py3k]",
   "language": "python",
   "name": "conda-env-py3k-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
