{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here we use a 3x4 grid world environment with deterministic transitions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from environment import standard_grid, negative_grid, print_values, print_policy\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting discount factor $\\gamma$ and tolerance Bellman Error $\\tau$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tau = 1e-3\n",
    "GAMMA = 0.9\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the actions\n",
    "<ol>\n",
    "<li> U = Up</li>\n",
    "<li> D = Down</li>\n",
    "<li> R = Right</li>\n",
    "<li> L = Left</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "actions = ('U', 'D', 'L', 'R') # here we consider deterministic policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env = standard_grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rewards for each state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rewards for each state:\n",
      "---------------------------\n",
      " 0.00| 0.00| 0.00| 1.00|\n",
      "---------------------------\n",
      " 0.00| 0.00| 0.00|-1.00|\n",
      "---------------------------\n",
      " 0.00| 0.00| 0.00| 0.00|\n"
     ]
    }
   ],
   "source": [
    "print(\"Rewards for each state:\")\n",
    "print_values(env.rewards, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "policy = {}\n",
    "for s in env.actions.keys():\n",
    "    policy[s] = np.random.choice(actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the Initial Random Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial random policy:\n",
      "---------------------------\n",
      "  U  |  R  |  D  |     |\n",
      "---------------------------\n",
      "  L  |     |  U  |     |\n",
      "---------------------------\n",
      "  L  |  D  |  D  |  U  |\n"
     ]
    }
   ],
   "source": [
    " \n",
    "print(\"Initial random policy:\")\n",
    "print_policy(policy, env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing the Value function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "V = {}\n",
    "states = env.all_states()\n",
    "for s in states:\n",
    "    if s in env.actions:\n",
    "        V[s] = np.random.random()\n",
    "    else:\n",
    "        #terminal state\n",
    "        V[s] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Value_Iteration(V,env,actions,GAMMA,tau):\n",
    "    start = time()\n",
    "    while True:\n",
    "        delta = 0\n",
    "        for s in states:\n",
    "            old_v = V[s]\n",
    "            # V(s) only has value if it's not a terminal state\n",
    "            if s in policy:\n",
    "                new_v = float('-inf')\n",
    "                for a in actions:\n",
    "                    env.set_state(s)\n",
    "                    r = env.execute(a)\n",
    "                    v = r + GAMMA * V[env.current_state()]\n",
    "                    if v > new_v:\n",
    "                        new_v = v\n",
    "                V[s] = new_v\n",
    "                delta = max(delta, np.abs(old_v - V[s]))\n",
    "        if delta < tau:\n",
    "            break\n",
    "    # find a policy that leads to optimal value function\n",
    "    for s in policy.keys():\n",
    "        best_a = None\n",
    "        best_value = float('-inf')\n",
    "        # loop through all possible actions to find the best current action\n",
    "        for a in actions:\n",
    "            env.set_state(s)\n",
    "            r = env.execute(a)\n",
    "            v = r + GAMMA * V[env.current_state()]\n",
    "            if v > best_value:\n",
    "                best_value = v\n",
    "                best_a = a\n",
    "        policy[s] = best_a\n",
    "\n",
    "    # our goal here is to verify that we get the same answer as with policy iteration\n",
    "    print('Time taken:', time() - start, 's')\n",
    "    print(\"values:\")\n",
    "    print_values(V, env)\n",
    "    print(\"policy:\")\n",
    "    print_policy(policy, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def Policy_Iteration(V,env,actions,GAMMA,tau):\n",
    "    start = time()\n",
    "    while True:\n",
    "\n",
    "        # policy evaluation step - we already know how to do this!\n",
    "        while True:\n",
    "            delta = 0\n",
    "            for s in states:\n",
    "                old_v = V[s]\n",
    "\n",
    "            # V(s) only has value if it's not a terminal state\n",
    "                if s in policy:\n",
    "                    a = policy[s]\n",
    "                    env.set_state(s)\n",
    "                    r = env.execute(a)\n",
    "                    V[s] = r + GAMMA * V[env.current_state()]\n",
    "                    delta = max(delta, np.abs(old_v - V[s]))\n",
    "\n",
    "            if delta < tau:\n",
    "                break\n",
    "\n",
    "        # policy improvement step\n",
    "        convergence_flag = True\n",
    "        for s in states:\n",
    "            if s in policy:\n",
    "                old_a = policy[s]\n",
    "                new_a = None\n",
    "                best_value = float('-inf')\n",
    "            # loop through all possible actions to find the best current action\n",
    "                for a in actions:\n",
    "                    env.set_state(s)\n",
    "                    r = env.execute(a)\n",
    "                    v = r + GAMMA * V[env.current_state()]\n",
    "                    if v > best_value:\n",
    "                        best_value = v\n",
    "                        new_a = a\n",
    "                        policy[s] = new_a\n",
    "            if new_a != old_a:\n",
    "                convergence_flag = False\n",
    "\n",
    "        if convergence_flag:\n",
    "            break\n",
    "    print('Time taken:', time() - start, 's')\n",
    "\n",
    "    print(\"\\nFinal values:\")\n",
    "    print_values(V, env)\n",
    "    print(\"\\nFinal policy:\")\n",
    "    print_policy(policy, env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results for $\\gamma =  0.9$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 0.0024001598358154297 s\n",
      "\n",
      "Final values:\n",
      "---------------------------\n",
      " 0.81| 0.90| 1.00| 0.00|\n",
      "---------------------------\n",
      " 0.73| 0.00| 0.90| 0.00|\n",
      "---------------------------\n",
      " 0.66| 0.73| 0.81| 0.73|\n",
      "\n",
      "Final policy:\n",
      "---------------------------\n",
      "  R  |  R  |  R  |     |\n",
      "---------------------------\n",
      "  U  |     |  U  |     |\n",
      "---------------------------\n",
      "  U  |  R  |  U  |  L  |\n"
     ]
    }
   ],
   "source": [
    "Policy_Iteration(V,env,actions,GAMMA,tau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 0.00020599365234375 s\n",
      "values:\n",
      "---------------------------\n",
      " 0.81| 0.90| 1.00| 0.00|\n",
      "---------------------------\n",
      " 0.73| 0.00| 0.90| 0.00|\n",
      "---------------------------\n",
      " 0.66| 0.73| 0.81| 0.73|\n",
      "policy:\n",
      "---------------------------\n",
      "  R  |  R  |  R  |     |\n",
      "---------------------------\n",
      "  U  |     |  U  |     |\n",
      "---------------------------\n",
      "  U  |  R  |  U  |  L  |\n"
     ]
    }
   ],
   "source": [
    "Value_Iteration(V,env,actions,GAMMA,tau)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results for $\\gamma =  0.5$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Gamma2 = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 0.00022602081298828125 s\n",
      "\n",
      "Final values:\n",
      "---------------------------\n",
      " 0.25| 0.50| 1.00| 0.00|\n",
      "---------------------------\n",
      " 0.12| 0.00| 0.50| 0.00|\n",
      "---------------------------\n",
      " 0.06| 0.12| 0.25| 0.12|\n",
      "\n",
      "Final policy:\n",
      "---------------------------\n",
      "  R  |  R  |  R  |     |\n",
      "---------------------------\n",
      "  U  |     |  U  |     |\n",
      "---------------------------\n",
      "  U  |  R  |  U  |  L  |\n"
     ]
    }
   ],
   "source": [
    "Policy_Iteration(V,env,actions,Gamma2,tau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 0.00021409988403320312 s\n",
      "values:\n",
      "---------------------------\n",
      " 0.25| 0.50| 1.00| 0.00|\n",
      "---------------------------\n",
      " 0.12| 0.00| 0.50| 0.00|\n",
      "---------------------------\n",
      " 0.06| 0.12| 0.25| 0.12|\n",
      "policy:\n",
      "---------------------------\n",
      "  R  |  R  |  R  |     |\n",
      "---------------------------\n",
      "  U  |     |  U  |     |\n",
      "---------------------------\n",
      "  U  |  R  |  U  |  L  |\n"
     ]
    }
   ],
   "source": [
    "Value_Iteration(V,env,actions,Gamma2,tau)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results for $\\gamma =  0.4$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Gamma3 = 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 0.00020885467529296875 s\n",
      "\n",
      "Final values:\n",
      "---------------------------\n",
      " 0.16| 0.40| 1.00| 0.00|\n",
      "---------------------------\n",
      " 0.06| 0.00| 0.40| 0.00|\n",
      "---------------------------\n",
      " 0.03| 0.06| 0.16| 0.06|\n",
      "\n",
      "Final policy:\n",
      "---------------------------\n",
      "  R  |  R  |  R  |     |\n",
      "---------------------------\n",
      "  U  |     |  U  |     |\n",
      "---------------------------\n",
      "  U  |  R  |  U  |  L  |\n"
     ]
    }
   ],
   "source": [
    "Policy_Iteration(V,env,actions,Gamma3,tau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 0.0003390312194824219 s\n",
      "values:\n",
      "---------------------------\n",
      " 0.16| 0.40| 1.00| 0.00|\n",
      "---------------------------\n",
      " 0.06| 0.00| 0.40| 0.00|\n",
      "---------------------------\n",
      " 0.03| 0.06| 0.16| 0.06|\n",
      "policy:\n",
      "---------------------------\n",
      "  R  |  R  |  R  |     |\n",
      "---------------------------\n",
      "  U  |     |  U  |     |\n",
      "---------------------------\n",
      "  U  |  R  |  U  |  L  |\n"
     ]
    }
   ],
   "source": [
    "Value_Iteration(V,env,actions,Gamma3,tau)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comments on Results \n",
    "We can draw the following conclusions from the results:\n",
    "<ol>\n",
    "<p><div style=\"text-align: justify\"> <li><I>Rate of Convergence:</I></li>\n",
    "       As seen from the convergence rates Policy Iteration generally converges to optimal policies faster than Value Iteration. This is because modified policy iteration uses one step value update for every policy improvement step. </div> </p>\n",
    "<p><div style=\"text-align: justify\"> <li><I>Uniqueness of Policy and Value Functions:</I></li>\n",
    "Both Policy Iteration and Value Iteration converge to same Optimal Policies and Values corresponding to the fixed point .</p>\n",
    "<p><div style=\"text-align: justify\"> <li><I>Effect of discount factor ($\\gamma$) on Value Function and Policy:</I></li>\n",
    "Changing the value of discount factor ($\\gamma$) causes the Bellman Operator converge to different fixed points resulting different Optimal Policies and Values for different values of $\\gamma$.</p>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py3k]",
   "language": "python",
   "name": "conda-env-py3k-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
